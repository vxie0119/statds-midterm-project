---
title: "Midterm Project"
author: "Vincent Xie"
toc: true
number-sections: true
highlight-style: pygments
format: 
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

# **Midterm Project: Rodents in NYC** 

[Rodents in NYC](https://en.wikipedia.org/wiki/Rats_in_New_York_City)
are widespread, as they are in many densely populated areas. As of
October 2023, NYC dropped from the 2nd to the 3rd places in the annual
["rattiest city"
list](https://www.orkin.com/press-room/top-rodent-infested-cities-2023#)
released by a pest control company. Rat sightings in NYC was analyzed
by Dr. Michael Walsh in a [2014 PeerJ
article](https://peerj.com/articles/533/). We investigate this problem
from a different angle with the [NYC Rodent Inspection
data](https://data.cityofnewyork.us/Health/Rodent-Inspection/p937-wjvj/about_data),
provided by the Department of Health and Mental Hygiene (DOHMH).
Download the 2022-2023 data by filtering the `INSPECTION_DATE` to
between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024
and `INSPECTION_TYPE` is either `Initial` or `Compliance` (which
should be about 108 MB). Read the meta data information
to understand the data.

## Data Initialization
```{python}
# Importing packages and dataset
import pandas as pd
import pyarrow as pa
import pyarrow.feather as feather
import os
from uszipcode import SearchEngine
import warnings
from arcgis.geocoding import reverse_geocode
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut

# Suppress all warnings
warnings.filterwarnings('ignore')

data = pd.read_csv("./data/Rodent_Inspection_Data.csv")
pluto = pd.read_csv("./data/PLUTO.csv")

# Listing the columns in vertical order
columns = data.columns.to_series().reset_index(drop=True)
columns
```

## Data cleaning

**There are two zipcode columns: `ZIP_CODE` and `Zipcodes`. Which one represent the zipcode of the inspection site? Comment on the data dictionary.**

The column `ZIP_CODE` represents the actual zipcode of the inspection site. When reviewing some of the `Zipcodes` values, I saw that some of them were from different states. We see that `Zipcodes` may be based off where the building owner is located. Given the representation of the value, it would be best to remove the `Zipcodes` column as it is not important in the overall investigation. 

Some key notes in the data dictionary:

- If a property/taxlot does not appear on file, it does not indicate the absense of rats.
- Neighborhoods with higher numbers of properties with active rat signs may not have higher rat populations. Just simply more inspections.
- `INSPECTION_TYPE` and `JOB_TICKET_OR_WORK_ORDER_ID` are unique identifiers for records in the dataset. 
- `JOB_ID` is not a unique value and two values may be equal depending on if it is an `Initial` inspection or `Compliance` inspection. 

```{python}
is_unique = data['JOB_ID'].is_unique
is_unique
```

- `JOB_PROGRESS` aligns with `INSPECTION_TYPE` where Initial = 1 and Compliance = 2.
- `BBL` is a number using `BORO_CODE`, `BLOCK`, and `LOT`. If any `BBL` values are missing, we can generate the values based on the three other values. This dataset can also be mapped using the Department of City Planning's PLUTO dataset which could be used to fill in missing data.
    - Common columns between Inspection data and PLUTO data: borough, block, lot, community board, census tract, council district, postcode/zipcode, police precinct, bbl, xcoord, ycoord, latitude, longitude.
- `X/Y_COORDINATE` use the NY State Plane Long Island Coordinate system.
- `Latitude/Longitude` is in decimal degrees of the World Geodetic System (WGS84).


**Summarize the missing information. Are their missing values that can be filled using other columns? Fill in those values.**
```{python}
# Checking for missing values in each column
missing_info = data.isnull().sum()

# Summarizing the missing information
missing_info_summary = missing_info[missing_info > 0].sort_values(ascending=False)
missing_info_summary
```

- BBL: The Borough, Block, Lot number (BBL) has the same number of missing values as the Building Identification Number (BIN). We see that there are no missing values for `BORO_CODE`, `BLOCK`, and `LOT` so we can use these values to fill in the BBL.

```{python}
# Filling in every row in the BBL column with the boro_code, block, and lot numbers
# The BBL is a concatenation of the borough code, block, and lot number

# Function to concatenate boro_code, block, and lot to create the BBL
def create_bbl(boro_code, block, lot):
    return int(f"{boro_code}{block:05d}{lot:04d}")

# Applying the function to each row where BBL is missing
data.loc[data['BBL'].isnull(), 'BBL'] = data.apply(lambda row: create_bbl(row['BORO_CODE'],
                                                   row['BLOCK'], row['LOT']) 
                                                   if pd.isnull(row['BBL']) 
                                                   else row['BBL'], axis=1)

# Convert the entire BBL column to integer
data['BBL'] = data['BBL'].astype(int)

# Checking the first few rows to verify the changes
print(data[['BORO_CODE', 'BLOCK', 'LOT', 'BBL']].head())

# Save the modified dataset
data.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- X_COORD and Y_COORD: These coordinates use the NY State Plane Long Island Coordinate System. I also noted that these coordinates may also be contained within the PLUTO dataset. Because we just filled in the BBL in the Inspection dataset, it will be useful in our search to fill in the coordinates. 

```{python}
# Filling X/Y Coordinates

clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Ensure BBL is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on BBL
merged_data = pd.merge(clean, pluto[['bbl', 'xcoord', 'ycoord']], 
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing X_coord and Y_coord values
merged_data['X_COORD'] = merged_data['X_COORD'].fillna(merged_data['xcoord'])
merged_data['Y_COORD'] = merged_data['Y_COORD'].fillna(merged_data['ycoord'])

# Convert X_COORD and Y_COORD to integers
merged_data['X_COORD'] = merged_data['X_COORD'].astype(int, errors='ignore')
merged_data['Y_COORD'] = merged_data['Y_COORD'].astype(int, errors='ignore')

# Fill NaN values with a placeholder (e.g., -1) and then convert to integers
# Otherwise, all of the values will have a decimal point after
merged_data['X_COORD'] = merged_data['X_COORD'].fillna(-1).astype(int)
merged_data['Y_COORD'] = merged_data['Y_COORD'].fillna(-1).astype(int)

# Drop the extra columns from PLUTO
merged_data.drop(['bbl', 'xcoord', 'ycoord'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- Zipcodes: Within this column, there are 31,114 rows with missing values. Not only that, the information does not serve a purpose to our investigation considering it was not mentioned in the data dictionary. With this in mind, it would be best to remove the column to free up some storage space.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Dropping Zipcodes column
clean.drop('Zip Codes', axis = 1, inplace = True)

# Fill NaN values in ZIP_CODE column (if any) with a placeholder (e.g., -1)
# Use an appropriate placeholder that makes sense for your data
clean['ZIP_CODE'] = clean['ZIP_CODE'].fillna(-1)

# Convert ZIP_CODE to integer
clean['ZIP_CODE'] = clean['ZIP_CODE'].astype(int)

# Save the updated dataset back to CSV
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- Borough Boundaries: From the data, we see that the `Borough Boundaries` column has a number associated with a specific borough. We see that:

    - Staten Island = 1
    - Brooklyn = 2
    - Queens = 3 
    - Manhattan = 4
    - Bronx = 5

The values in this column can easily be filled in based on what the `Borough` value is.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Mapping of boroughs to their boundary numbers
borough_mapping = {
    'Staten Island': 1,
    'Brooklyn': 2,
    'Queens': 3,
    'Manhattan': 4,
    'Bronx': 5
}

# Filling in the 'Borough Boundary' column based on the 'Borough' column
clean['Borough Boundaries'] = clean['BOROUGH'].map(borough_mapping)

# Convert 'Borough Boundary' to integer
# Here, NaN values (if any) will prevent conversion to integers, so you might need to fill them first
clean['Borough Boundaries'] = clean['Borough Boundaries'].fillna(-1).astype(int)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- Police Precinct, Community Districts, City Council District: The PLUTO dataset does not have a replacement for Community Districts and City Council District. These columns were dropped as they provide no significance to the investigation. Police Precinct was one of the common column names between the two datasets so I filled in the empty values based off of BBL number.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure BBL is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on BBL
merged_data = pd.merge(clean, pluto[['bbl', 'policeprct']], 
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing 'Police Precinct' values
merged_data['Police Precincts'] = merged_data['Police Precincts'].fillna(merged_data['policeprct'])

# Convert 'Police Precinct' to integer
merged_data['Police Precincts'] = merged_data['Police Precincts'].fillna(-1).astype(int)

# Drop the extra columns
merged_data.drop(['Community Districts', 'City Council Districts', 'bbl', 'policeprct'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- BIN: The Building Identification Number was not found in the PLUTO dataset. Although it seems like an important column to have, there are no columns that can be used to fill in the missing values. The best step would be to remove the column completely.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Dropping BIN column   
clean.drop('BIN', axis = 1, inplace = True)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- Community Board, Council District, Census Tract: There are 10,304 missing values in each of these columns. They can all be filled using the `community board`, `census tract 2010`, and `council district` columns from the PLUTO dataset. 

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure BBL is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on BBL
merged_data = pd.merge(clean, pluto[['bbl', 'community board', 'council district', 'census tract 2010']], 
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing values with the corresponding values from PLUTO
columns_to_update = {
    'COMMUNITY BOARD': 'community board',
    'COUNCIL DISTRICT': 'council district',
    'CENSUS TRACT': 'census tract 2010'
}

for clean_col, pluto_col in columns_to_update.items():
    merged_data[clean_col] = merged_data[clean_col].fillna(merged_data[pluto_col])

# Convert to integers
for col in columns_to_update.keys():
    merged_data[col] = merged_data[col].fillna(-1).astype(int)

# Drop the extra columns from PLUTO
merged_data.drop(['bbl', 'community board', 'council district', 'census tract 2010'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- NTA: The Neighborhood Tabulation Area cannot be filled using other columns. It is also missing from the PLUTO dataset. I believe that this column is important as it provides a more specific name to the inspection location but I cannot fill in the missing values. If the dataset had included a column with `NTACode`, it would be a lot easier to fill in the missing NTA values. This [dataset](https://data.cityofnewyork.us/City-Government/2010-Neighborhood-Tabulation-Areas-NTAs-/cpf4-rkhq) will have the corresponding `NTACode` with `NTA` to fill in any missing values.

- Location, Latitude, Longitude: There are not too many values that are missing but these values can all be filled in by using the BBL number to match the numbers. 

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure the BBL column is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on the BBL column
merged_data = pd.merge(clean, pluto[['bbl', 'latitude', 'longitude']],
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing latitude and longitude values
merged_data['LATITUDE'] = merged_data['LATITUDE'].fillna(merged_data['latitude'])
merged_data['LONGITUDE'] = merged_data['LONGITUDE'].fillna(merged_data['longitude']) 

# Create a 'LOCATION' column by combining LATITUDE and LONGITUDE
# Check if LATITUDE and LONGITUDE are not NaN before combining
merged_data['LOCATION'] = merged_data.apply(
    lambda row: f"({row['LATITUDE']}, {row['LONGITUDE']})"
    if pd.notnull(row['LATITUDE']) and pd.notnull(row['LONGITUDE'])
    else None, axis=1)

# Drop the extra columns from pluto_data that are no longer needed
merged_data.drop(['bbl', 'latitude', 'longitude'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- ZIP_CODE: By using information from the `Location` and  `Street Name` column, we can locate the zipcode using a geospatial python package.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Create a SearchEngine object for looking up ZIP codes
search = SearchEngine()

# Function to look up ZIP code by latitude and longitude
def get_zipcode(lat, lon):
    if lat == 0.0 and lon == 0.0:
        # Return 0 if both latitude and longitude are 0.0
        return 0
    result = search.by_coordinates(lat=lat, lng=lon, radius=30, returns=1)
    if result:
        return result[0].zipcode
    return None

# Iterate over the DataFrame rows and fill in missing ZIP codes
for index, row in clean.iterrows():
    # Check if ZIP_CODE is NaN, 0, or -1, and latitude/longitude are not both 0.0
    if pd.isnull(row['ZIP_CODE']) or row['ZIP_CODE'] in [-1]:
        if pd.notnull(row['LATITUDE']) and pd.notnull(row['LONGITUDE']):
            zip_code = get_zipcode(row['LATITUDE'], row['LONGITUDE'])
            clean.at[index, 'ZIP_CODE'] = zip_code

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- HOUSE_NUMBER and STREET_NAME: Using the code below, we are able to take the `Latitude` and `Longitude` values to find out the `STREET_NAME` and `HOUSE_NUMBER` if those attributes are available. A warning for this code is that it takes a significant amount of time to render. Because of the very large dataset, it took roughly 10-20 minutes to render everything.

```{python}
"""
# Initialize the geocoder
geolocator = Nominatim(user_agent="Project")

# Function to perform reverse geocoding
def reverse_geocode(lat, lon):
    try:
        # Get location information
        location = geolocator.reverse((lat, lon), exactly_one=True)
        if location:
            address = location.raw.get('address', {})
            # Extract house number and street name
            house_number = address.get('house_number', None)
            street_name = address.get('road', None)
            return house_number, street_name
    except GeocoderTimedOut:
        return None, None
    except Exception as e:
        print(f"Error: {e}")
        return None, None

# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Iterate over the DataFrame rows
for index, row in clean.iterrows():
    # Check if HOUSE_NUMBER and STREET_NAME need to be filled
    if pd.isnull(row['HOUSE_NUMBER']) or pd.isnull(row['STREET_NAME']):
        # Perform reverse geocoding if lat and lon are available
        if pd.notnull(row['LATITUDE']) and pd.notnull(row['LONGITUDE']):
            house_number, street_name = reverse_geocode(row['LATITUDE'], row['LONGITUDE'])
            clean.at[index, 'HOUSE_NUMBER'] = house_number if house_number else row['HOUSE_NUMBER']
            clean.at[index, 'STREET_NAME'] = street_name if street_name else row['STREET_NAME']

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
"""
```

- RESULT and BOROUGH: In terms of number of missing values, these two columns have very few. We can analyze them and see what is the potential issue and why it is missing.

```{python}
# Load your dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Display rows where the 'RESULT' column is empty (NaN)
empty_result_rows = clean[clean['RESULT'].isnull()]
print(empty_result_rows)
```

From the 21 instances where `RESULT` is empty, I noticed that majority of the other columns are filled. I also observed that the only `INSPECTION_TYPE` is Compliance. This is interesting because it means that on the second inspection, we have a missing result which may indicate that someone forgot to input the result.

```{python}
# Display rows where the 'RESULT' column is empty (NaN)
empty_borough_rows = clean[clean['BOROUGH'].isnull()]
print(empty_borough_rows)
```
With the 3 instances where `BOROUGH` is empty, it was weird to see both `X_COORD`, `Y_COORD`, `LATITUDE`, and `LONGITUDE` empty. There were also a bunch of other attributes that were empty (indicated by -1). Because there are very few instances to clean, you can manually clean it without using code. I find it compelling how street names are given with the zipcode but a `BOROUGH` was not inputted. 

**Data Cleaning Conclusion**

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Checking for missing values in each column
missing_info = clean.isnull().sum()

# Summarizing the missing information
missing_info_summary = missing_info[missing_info > 0].sort_values(ascending=False)
missing_info_summary
```

From this summary, the number of missing values have been reduced signifcantly and can be used to properly analyze the data.

- There are still 10,304 NTA values that are missing. The column itself can be removed but there is a way to retrieve the NTA value if NTACode was given.
- There are some `HOUSE_NUMBER` values that are missing because some of the rows reference an entire street and not necessarily one building.
- Likewise, `LOCATION`, `LATITUDE`, and `LONGITUDE` have some values that are missing because the `STREET_NAME` is referencing an *entire* street. 

**Are there redundant information in the data? Try storing the data using `arrow` and comment on the efficiency gain.**

After cleaning the data, you could say that `BBL` is redundant as the table already contains `BORO_CODE`, `BLOCK` and `LOT`. But from my cleaning, `BBL` played a crucial role in finding missing data. It identifies a specific taxlot that has information ranging from street name to zipcode. You could also say that `LATITUDE` and `LONGITUDE` is redundant because it is contained in `LOCATION`. I believe that it is good to keep both for specific use cases where you may need a `LOCATION` attribute. 

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Convert the Pandas DataFrame to an Arrow Table
table = pa.Table.from_pandas(clean)

# Save the Arrow Table as a Feather file
feather_file_path = './data/cleaned_rodent_inspection.feather'
feather.write_feather(table, feather_file_path)

# Check the size of the original CSV file and the new Feather file
original_size = os.path.getsize('./data/cleaned_rodent_inspection.csv')
feather_size = os.path.getsize(feather_file_path)

original_size, feather_size
```


**Are there invalid zipcode or borough? Justify and clean them up if yes.**



## Data exploration

**Create binary variable `passing` indicating passing or not for the inspection result. Does `passing` depend on whether the inspection is initial or compliance? State your hypothesis and summarize your test result.**

**Are the passing pattern different across different boroughs for initial inspections? How about compliance inspections? State your hypothesis and summarize your test results.**

**If we suspect that the passing rate may depends on the time of a day of the inspection, we may compare the passing rates for inspections done in the mornings and inspections one in the afternoons. Visualize the comparison by borough and inspection type.**

**Perform a formal hypothesis test to confirm the observations from your visualization.**

## Data analytics

**Aggregate the inspections by zip code to create a dataset with five columns. The first three columns are `zipcode`; `n_initial`, the count of the initial inspections in that zipcode; and  `n_initpass`, the number of initial inspections with a passing result in that zipcode. The other two variables are `n_compliance` and `n_comppass`, the counterpart for compliance inspections.**

**Add a variable to your dataset, `n_sighting`, which represent the number of rodent sightings from the 311 service request data in the same 2022-2023 period.**

**Merge your dataset with the simple zipcode table in package `uszipcode` by zipcode to obtain demographic and socioeconomic variables at the zipcode level.**

**Build a binomial regression for the passing rate of initial inspections at the zipcode level. Assess the goodness-of-fit of your model. Summarize your results to a New Yorker who is not data science savvy.**

##  Research Question

   Now you know the data quite well. Come up with a research
   question of interest that can be answered by the data, which
   could be analytics or visualizations. Perform the needed
   analyses and answer your question.

## References 

* [GeoPy reference](https://stackoverflow.com/questions/60928516/get-address-from-given-coordinate-using-python)
* [PLUTO data](https://data.cityofnewyork.us/City-Government/Primary-Land-Use-Tax-Lot-Output-PLUTO-/64uk-42ks/data)