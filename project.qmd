---
title: "Midterm Project"
author: "Vincent Xie"
toc: true
number-sections: true
highlight-style: pygments
format: 
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

# **Midterm Project: Rodents in NYC**   

[Rodents in NYC](https://en.wikipedia.org/wiki/Rats_in_New_York_City)
are widespread, as they are in many densely populated areas. As of
October 2023, NYC dropped from the 2nd to the 3rd places in the annual
["rattiest city"
list](https://www.orkin.com/press-room/top-rodent-infested-cities-2023#)
released by a pest control company. Rat sightings in NYC was analyzed
by Dr. Michael Walsh in a [2014 PeerJ
article](https://peerj.com/articles/533/). We investigate this problem
from a different angle with the [NYC Rodent Inspection
data](https://data.cityofnewyork.us/Health/Rodent-Inspection/p937-wjvj/about_data),
provided by the Department of Health and Mental Hygiene (DOHMH).
Download the 2022-2023 data by filtering the `INSPECTION_DATE` to
between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024
and `INSPECTION_TYPE` is either `Initial` or `Compliance` (which
should be about 108 MB). Read the meta data information
to understand the data.

## Data Initialization
```{python}
# Importing packages and dataset
import pandas as pd
from scipy.stats import chi2_contingency
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import numpy as np
from statsmodels.formula.api import ols
import time
# Storage Packages
import pyarrow as pa
import pyarrow.feather as feather
import os
# Geospatial Packages
from uszipcode import SearchEngine
import warnings
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError

# Disable Intel MKL warning
os.environ['MKL_DEBUG_CPU_TYPE'] = '5'

# Suppress all warnings
warnings.filterwarnings('ignore')

data = pd.read_csv("./data/Rodent_Inspection_Data.csv")
pluto = pd.read_csv("./data/PLUTO.csv")

# Listing the columns in vertical order
columns = data.columns.to_series().reset_index(drop=True)
columns
```

## Data cleaning

**There are two zipcode columns: `ZIP_CODE` and `Zipcodes`. Which one represent the zipcode of the inspection site? Comment on the data dictionary.**

The column `ZIP_CODE` represents the actual zipcode of the inspection site. When reviewing some of the `Zipcodes` values, I saw that some of them were from different states. We see that `Zipcodes` may be based off where the building owner is located. Given the representation of the value, it would be best to remove the `Zipcodes` column as it is not important in the overall investigation. 

Some key notes in the data dictionary:

- If a property/taxlot does not appear on file, it does not indicate the absense of rats.
- Neighborhoods with higher numbers of properties with active rat signs may not have higher rat populations. Just simply more inspections.
- `INSPECTION_TYPE` and `JOB_TICKET_OR_WORK_ORDER_ID` are unique identifiers for records in the dataset. 
- `JOB_ID` is not a unique value and two values may be equal depending on if it is an `Initial` inspection or `Compliance` inspection. 

```{python}
is_unique = data['JOB_ID'].is_unique
is_unique
```

- `JOB_PROGRESS` aligns with `INSPECTION_TYPE` where Initial = 1 and Compliance = 2.
- `BBL` is a number using `BORO_CODE`, `BLOCK`, and `LOT`. If any `BBL` values are missing, we can generate the values based on the three other values. This dataset can also be mapped using the Department of City Planning's PLUTO dataset which could be used to fill in missing data.
    - Common columns between Inspection data and PLUTO data: borough, block, lot, community board, census tract, council district, postcode/zipcode, police precinct, bbl, xcoord, ycoord, latitude, longitude.
- `X/Y_COORDINATE` use the NY State Plane Long Island Coordinate system.
- `Latitude/Longitude` is in decimal degrees of the World Geodetic System (WGS84).


**Summarize the missing information. Are their missing values that can be filled using other columns? Fill in those values.**
```{python}
# Checking for missing values in each column
missing_info = data.isnull().sum()

# Summarizing the missing information
missing_info_summary = missing_info[missing_info > 0].sort_values(ascending=False)
missing_info_summary
```

- BBL: The Borough, Block, Lot number (BBL) has the same number of missing values as the Building Identification Number (BIN). We see that there are no missing values for `BORO_CODE`, `BLOCK`, and `LOT` so we can use these values to fill in the BBL.

```{python}
# Filling in every row in the BBL column with the boro_code, block, and lot numbers
# The BBL is a concatenation of the borough code, block, and lot number

# Function to concatenate boro_code, block, and lot to create the BBL
def create_bbl(boro_code, block, lot):
    return int(f"{boro_code}{block:05d}{lot:04d}")

# Applying the function to each row where BBL is missing
data.loc[data['BBL'].isnull(), 'BBL'] = data.apply(lambda row: create_bbl(\
                                                   row['BORO_CODE'],
                                                   row['BLOCK'], row['LOT']) 
                                                   if pd.isnull(row['BBL']) 
                                                   else row['BBL'], axis=1)

# Convert the entire BBL column to integer
data['BBL'] = data['BBL'].astype(int)

# Checking the first few rows to verify the changes
print(data[['BORO_CODE', 'BLOCK', 'LOT', 'BBL']].head())

# Save the modified dataset
data.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- X_COORD and Y_COORD: These coordinates use the NY State Plane Long Island Coordinate System. I also noted that these coordinates may also be contained within the PLUTO dataset. Because we just filled in the BBL in the Inspection dataset, it will be useful in our search to fill in the coordinates. 

```{python}
# Filling X/Y Coordinates

clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Ensure BBL is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on BBL
merged_data = pd.merge(clean, pluto[['bbl', 'xcoord', 'ycoord']], 
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing X_coord and Y_coord values
merged_data['X_COORD'] = merged_data['X_COORD'].fillna(merged_data['xcoord'])
merged_data['Y_COORD'] = merged_data['Y_COORD'].fillna(merged_data['ycoord'])

# Convert X_COORD and Y_COORD to integers
merged_data['X_COORD'] = merged_data['X_COORD'].astype(int, errors='ignore')
merged_data['Y_COORD'] = merged_data['Y_COORD'].astype(int, errors='ignore')

# Fill NaN values with a placeholder (e.g., 0) and then convert to integers
# Otherwise, all of the values will have a decimal point after
merged_data['X_COORD'] = merged_data['X_COORD'].fillna(0).astype(int)
merged_data['Y_COORD'] = merged_data['Y_COORD'].fillna(0).astype(int)

# Drop the extra columns from PLUTO
merged_data.drop(['bbl', 'xcoord', 'ycoord'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- Zipcodes: Within this column, there are 31,114 rows with missing values. Not only that, the information does not serve a purpose to our investigation considering it was not mentioned in the data dictionary. With this in mind, it would be best to remove the column to free up some storage space.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Dropping Zipcodes column
clean.drop('Zip Codes', axis = 1, inplace = True)

# Save the updated dataset back to CSV
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- Borough Boundaries: From the data, we see that the `Borough Boundaries` column has a number associated with a specific borough. We see that:

    - Staten Island = 1
    - Brooklyn = 2
    - Queens = 3 
    - Manhattan = 4
    - Bronx = 5

The values in this column can easily be filled in based on what the `Borough` value is.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Mapping of boroughs to their boundary numbers
borough_mapping = {
    'Staten Island': 1,
    'Brooklyn': 2,
    'Queens': 3,
    'Manhattan': 4,
    'Bronx': 5
}

# Filling in the 'Borough Boundary' column based on the 'Borough' column
clean['Borough Boundaries'] = clean['BOROUGH'].map(borough_mapping)

# Convert 'Borough Boundary' to integer
# Here, NaN values (if any) will prevent conversion to integers, 
# so you might need to fill them first
clean['Borough Boundaries'] = clean['Borough Boundaries'].fillna(-1).astype(int)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- Police Precinct, Community Districts, City Council District: The PLUTO dataset does not have a replacement for Community Districts and City Council District. These columns were dropped as they provide no significance to the investigation. Police Precinct was one of the common column names between the two datasets so I filled in the empty values based off of BBL number.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure BBL is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on BBL
merged_data = pd.merge(clean, pluto[['bbl', 'policeprct']], 
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing 'Police Precinct' values
merged_data['Police Precincts'] = merged_data['Police Precincts'].fillna(merged_data
                                                                         ['policeprct'])

# Convert 'Police Precinct' to integer
merged_data['Police Precincts'] = merged_data['Police Precincts'].fillna(0).astype(int)

# Drop the extra columns
merged_data.drop(['Community Districts', 'City Council Districts', 'bbl', 'policeprct'],
                   axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- BIN: The Building Identification Number was not found in the PLUTO dataset. Although it seems like an important column to have, there are no columns that can be used to fill in the missing values. The best step would be to remove the column completely.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Dropping BIN column   
clean.drop('BIN', axis = 1, inplace = True)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- Community Board, Council District, Census Tract: There are 10,304 missing values in each of these columns. They can all be filled using the `community board`, `census tract 2010`, and `council district` columns from the PLUTO dataset. 

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure BBL is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on BBL
merged_data = pd.merge(clean, pluto[['bbl', 'community board', 'council district', 
                                     'census tract 2010']], 
                                     left_on='BBL', right_on='bbl', how='left')

# Fill in the missing values with the corresponding values from PLUTO
columns_to_update = {
    'COMMUNITY BOARD': 'community board',
    'COUNCIL DISTRICT': 'council district',
    'CENSUS TRACT': 'census tract 2010'
}

for clean_col, pluto_col in columns_to_update.items():
    # Only update cells that are NaN in the clean dataset with PLUTO dataset values
    merged_data.loc[merged_data[clean_col].isna(), clean_col] = merged_data[pluto_col]

# Convert to integers
for col in columns_to_update.keys():
    merged_data[col] = merged_data[col].fillna(0).astype(int)

# Drop the extra columns from PLUTO
merged_data.drop(['bbl', 'community board', 'council district', 
                  'census tract 2010'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- NTA: The Neighborhood Tabulation Area cannot be filled using other columns. It is also missing from the PLUTO dataset. I believe that this column is important as it provides a more specific name to the inspection location but I cannot fill in the missing values. If the dataset had included a column with `NTACode`, it would be a lot easier to fill in the missing NTA values. This [dataset](https://data.cityofnewyork.us/City-Government/2010-Neighborhood-Tabulation-Areas-NTAs-/cpf4-rkhq) will have the corresponding `NTACode` with `NTA` to fill in any missing values.

- Location, Latitude, Longitude: There are not too many values that are missing but these values can all be filled in by using the BBL number to match the numbers. 

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure the BBL column is the same data type in both datasets
clean['BBL'] = clean['BBL'].astype(int)
pluto['bbl'] = pluto['bbl'].astype(int)

# Merge the datasets on the BBL column
merged_data = pd.merge(clean, pluto[['bbl', 'latitude', 'longitude']],
                       left_on='BBL', right_on='bbl', how='left')

# Fill in the missing latitude and longitude values
merged_data['LATITUDE'] = merged_data['LATITUDE'].fillna(merged_data['latitude'])
merged_data['LONGITUDE'] = merged_data['LONGITUDE'].fillna(merged_data['longitude']) 

# Create a 'LOCATION' column by combining LATITUDE and LONGITUDE
# Check if LATITUDE and LONGITUDE are not NaN before combining
merged_data['LOCATION'] = merged_data.apply(
    lambda row: f"({row['LATITUDE']}, {row['LONGITUDE']})"
    if pd.notnull(row['LATITUDE']) and pd.notnull(row['LONGITUDE'])
    else None, axis=1)

# Drop the extra columns from pluto_data that are no longer needed
merged_data.drop(['bbl', 'latitude', 'longitude'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
```

- ZIP_CODE: By using information from the `Location` and  `Street Name` column, we can locate the zipcode using a geospatial python package.

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Ensure 'BBL' column in both DataFrames is of the same type
clean['BBL'] = clean['BBL'].astype(str)
pluto['bbl'] = pluto['bbl'].astype(str)



# Merge the datasets on the 'BBL' column
merged_data = pd.merge(clean, pluto[['bbl', 'postcode']], left_on='BBL', right_on='bbl', how='left')

# Replace 'ZIP_CODE' values that are -1 or 0 with NaN to prepare for update
merged_data['ZIP_CODE'] = merged_data['ZIP_CODE'].replace([-1, 0], pd.NA)

# Update 'ZIP_CODE' in clean where it is NaN, with 'postcode' from pluto
merged_data['ZIP_CODE'] = merged_data['ZIP_CODE'].combine_first(merged_data['postcode'])

# Ensure 'ZIP_CODE' has no decimals and is of string type
merged_data['ZIP_CODE'] = merged_data['ZIP_CODE'].astype(str)

# Drop the 'bbl' and 'postcode' columns as they are no longer needed
merged_data.drop(['bbl', 'postcode'], axis=1, inplace=True)

# Save the merged dataset to a new CSV file
merged_data.to_csv("./data/cleaned_rodent_inspection.csv", index=False)
```

- HOUSE_NUMBER and STREET_NAME: Using the code below, we are able to take the `Latitude` and `Longitude` values to find out the `STREET_NAME` and `HOUSE_NUMBER` if those attributes are available. A warning for this code is that it takes a significant amount of time to render. Because of the very large dataset, it took roughly 10-20 minutes to render everything.

```{python}
'''
# Initialize the geocoder
geolocator = Nominatim(user_agent="Project")

# Function to perform reverse geocoding
def reverse_geocode(lat, lon):
    try:
        # Get location information
        location = geolocator.reverse((lat, lon), exactly_one=True, timeout = 3)
        if location:
            address = location.raw.get('address', {})
            # Extract house number and street name
            house_number = address.get('house_number', None)
            street_name = address.get('road', None)
            return house_number, street_name
    except GeocoderTimedOut:
        time.sleep(1)
        return reverse_geocode(lat, lon)
    except GeocoderServiceError as e:
        print(f"Service Error: {e}")
        return None, None
    except Exception as e:
        print(f"Error: {e}")
        return None, None

# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Iterate over the DataFrame rows
for index, row in clean.iterrows():
    # Check if HOUSE_NUMBER and STREET_NAME need to be filled
    if pd.isnull(row['HOUSE_NUMBER']) or pd.isnull(row['STREET_NAME']):
        # Perform reverse geocoding if lat and lon are available
        if pd.notnull(row['LATITUDE']) and pd.notnull(row['LONGITUDE']):
            house_number, street_name = reverse_geocode(row['LATITUDE'], 
                                                        row['LONGITUDE'])
            clean.at[index, 'HOUSE_NUMBER'] = house_number if house_number \
                                                           else row['HOUSE_NUMBER']
            clean.at[index, 'STREET_NAME'] = street_name if street_name \
                                                         else row['STREET_NAME']
            time.sleep(1) # Delay between requests to avoid hitting rate limit
            
# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)
'''
```


- RESULT and BOROUGH: In terms of number of missing values, these two columns have very few. We can analyze them and see what is the potential issue and why it is missing.

```{python}
# Load your dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Display rows where the 'RESULT' column is empty (NaN)
empty_result_rows = clean[clean['RESULT'].isnull()]
print(empty_result_rows.head())
```

From the 21 instances where `RESULT` is empty, I noticed that majority of the other columns are filled. I also observed that the only `INSPECTION_TYPE` is Compliance. This is interesting because it means that on the second inspection, we have a missing result which may indicate that someone forgot to input the result.

```{python}
# Display rows where the 'RESULT' column is empty (NaN)
empty_borough_rows = clean[clean['BOROUGH'].isnull() | (clean['RESULT'] == 0)]
print(empty_borough_rows.head())
```

With the 3 instances where `BOROUGH` is empty, it was weird to see both `X_COORD`, `Y_COORD`, `LATITUDE`, and `LONGITUDE` empty. There were also a bunch of other attributes that were empty (indicated by -1). Because there are very few instances to clean, you can manually clean it without using code. I find it compelling how street names are given with the zipcode but a `BOROUGH` was not inputted. 

**Data Cleaning Conclusion**

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Define the columns where zeros should not be considered as missing data
exclude_columns = ['BLOCK', 'LOT']

# Function to check for missing values or zeros
def missing_or_zero(series):
    if series.name in exclude_columns:
        # For 'BLOCK' and 'LOT', only count NaNs as missing
        return series.isnull().sum()
    else:
        # For other columns, count both NaNs and zeros as missing
        return series.isnull().sum() + (series == 0).sum()

# Checking for missing values and zeros in each column, except for 'BLOCK' and 'LOT'
missing_info = clean.apply(missing_or_zero)

# Summarizing the missing information
missing_info_summary = missing_info[missing_info > 0].sort_values(ascending=False)
missing_info_summary
```

From this summary, the number of missing values have been reduced signifcantly and can be used to properly analyze the data.

- There are still 10,304 NTA values that are missing. The column itself can be removed but there is a way to retrieve the NTA value if NTACode was given.
- There are some `HOUSE_NUMBER` values that are missing because some of the rows reference an entire street and not necessarily one building.
- Likewise, `LOCATION`, `LATITUDE`, and `LONGITUDE` have some values that are missing because the `STREET_NAME` is referencing an *entire* street. 

**Are there redundant information in the data? Try storing the data using `arrow` and comment on the efficiency gain.**

After cleaning the data, you could say that `BBL` is redundant as the table already contains `BORO_CODE`, `BLOCK` and `LOT`. But from my cleaning, `BBL` played a crucial role in finding missing data. It identifies a specific taxlot that has information ranging from street name to zipcode. You could also say that `LATITUDE` and `LONGITUDE` is redundant because it is contained in `LOCATION`. I believe that it is good to keep both for specific use cases where you may need a `LOCATION` attribute. 

```{python}
# Reload the dataframe so updates pass through
clean = pd.read_csv("./data/cleaned_rodent_inspection.csv")

# Convert the Pandas DataFrame to an Arrow Table
table = pa.Table.from_pandas(clean)

# Save the Arrow Table as a Feather file
feather_file_path = './data/cleaned_rodent_inspection.feather'
feather.write_feather(table, feather_file_path)

# Check the size of the original CSV file and the new Feather file
original_size = os.path.getsize('./data/cleaned_rodent_inspection.csv')
feather_size = os.path.getsize(feather_file_path)

original_size, feather_size
```

Even after cleaning the data, storing the data using `arrow` is more efficient. Dropping from 104.9 MB to 67.24MB is pretty significant considering the large dataset.

**Are there invalid zipcode or borough? Justify and clean them up if yes.**

```{python}
# Load the dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Function to check if a ZIP code is valid based on New York City Neighborhoods
def is_valid_zip(zip_code):
    try:
        zip_int = int(float(zip_code))  # Convert to integer to remove any decimals
        return ((10001 <= zip_int <= 10282) or  # Manhattan
                (10301 <= zip_int <= 10314) or  # Staten Island
                (10451 <= zip_int <= 10475) or  # Bronx
                (11004 <= zip_int <= 11109) or  # Queens
                (11351 <= zip_int <= 11697) or  # Queens
                (11201 <= zip_int <= 11256))    # Brooklyn
    except ValueError:  # Handles cases where conversion to int might fail if the input is not a number
        return False

# Identifying invalid zip codes
invalid_zip_codes = clean[~clean['ZIP_CODE'].apply(is_valid_zip)]

# Identifying invalid boroughs
known_boroughs = ["Manhattan", "Brooklyn", "Queens", "Bronx", "Staten Island"]
invalid_boroughs = clean[~clean['BOROUGH'].isin(known_boroughs) & clean['BOROUGH'].notnull()]

# Counting invalid zip codes and boroughs
invalid_zip_codes_count = invalid_zip_codes.shape[0]
invalid_boroughs_count = invalid_boroughs.shape[0]

print(f"Invalid ZIP Codes: {invalid_zip_codes_count}")
print(f"Invalid Boroughs: {invalid_boroughs_count}")
```

We see that there are 3,239 invalid ZIP codes. This is mainly due to the fact that there are ZIP codes that have not been found. This was found to be quite challenging given that the `STREET_NAME` was an entire street so the ZIP codes may vary. It is safe to say that we do not have an invalid borough instance. 

In regards to the invalid ZIP codes, we see that there are some empty ZIP codes (denoted by 0). Some of these ZIP codes are quite difficult to find because the row provides information on a whole street. With a whole street, the ZIP codes will not be constant. 


## Data exploration

**Create binary variable `passing` indicating passing or not for the inspection result. Does `passing` depend on whether the inspection is initial or compliance? State your hypothesis and summarize your test result.**

```{python}
# Load the dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Define your criteria for 'passing' and categorize the 'passing' column
clean['passing'] = clean['RESULT'].apply(lambda x: 1 if x == 'Passed' else 0)

# Create a contingency table
contingency_table = pd.crosstab(clean['passing'], clean['INSPECTION_TYPE'])

# Perform the Chi-Squared Test
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)

# Summarize the results
print(contingency_table)
print(f"Chi-Squared Value: {chi2}")
print(f"P-Value: {p}")
```

**Analysis** 

| Null Hypothesis: There is no association between the `Inspection_Type` and the `Result`.
| Alternative Hypothesis: There is an association between the `Inspection_Type` and the `Result`.

Our results tell us that our Chi-Squared value is 29,443.398 with a p-value = 0.0 (we can say that p is a very small number less than 0.0001). If we are using $\alpha$ = 0.05, we can reject the null hypothesis and say that there is a statistically significant association between inspection type and the inspection result. It seems that the initial inspections have a higher proportion of passing results compared to compliance inspections. But, without knowing the specific criteria used to determine inspection results, we can't specify the cause of association.

**Are the passing pattern different across different boroughs for initial inspections? How about compliance inspections? State your hypothesis and summarize your test results.**

```{python}
# Load the dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Define your criteria for 'passing' and categorize the 'passing' column
clean['passing'] = clean['RESULT'].apply(lambda x: 1 if x == 'Passed' else 0)

# Chi-Squared Test for Initial Inspections
initial = clean[clean['INSPECTION_TYPE'] == 'Initial']
initial_table = pd.crosstab(initial['BOROUGH'], initial['passing'])
chi2_initial, p_initial, dof_initial, expected_initial = chi2_contingency(initial_table)

# Chi-Squared Test for Compliance Inspections
compliance = clean[clean['INSPECTION_TYPE'] == 'Compliance']
compliance_table = pd.crosstab(compliance['BOROUGH'], compliance['passing'])

chi2_comp, p_comp, dof_comp, expected_comp= chi2_contingency(compliance_table)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)

# Results
print('Initial Inspections:')
print(initial_table)
print(chi2_initial, p_initial)

print('Compliance Inspections:')
print(compliance_table)
print(chi2_comp, p_comp)
```

**Analysis** 

| Null Hypotheses for initial/compliance inspections: There is no assocation between the borough and the passing rates for inspections.
| Alternative Hypotheses for initial/compliance inspections: There is an association between the borough and the passing rates for inspections. 

For the Initial inspections, we have a Chi-Squared value of 2880.85 and a p-value of 0.0 (assumed that it is a very small number). We can reject the null hypothesis and say that there is a statistically significant association between boroughs and passing rates for initial inspections. We can say that the passing patterns vary between boroughs.

Similarly, our Compliance inspections have a Chi-Squared value of 294.33 and a p-value of 1.813E-62 (a very small number close to zero). We can also reject the null hypothesis and say that there is a statistically significant association between boroughs and passing rates. The passing patterns also vary between boroughs.

These tests suggest that the location where an inspection occurs has an impact on whether or not its likely to pass.

**If we suspect that the passing rate may depends on the time of a day of the inspection, we may compare the passing rates for inspections done in the mornings and inspections one in the afternoons. Visualize the comparison by borough and inspection type.**

```{python}
# Load the dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Convert inspection time to datetime (if it's not already)
# and create 'TIME_OF_DAY' category based on the hour of the inspection
clean['INSPECTION_DATE'] = pd.to_datetime(clean['INSPECTION_DATE'])
clean['TIME_OF_DAY'] = clean['INSPECTION_DATE'].apply(lambda x: 'Morning' 
                                                      if x.hour < 12 
                                                      else 'Afternoon')

# Define your criteria for 'passing' and categorize the 'passing' column
clean['passing'] = clean['RESULT'].apply(lambda x: 1 if x == 'Passed' else 0)

# Calculate the passing rate by borough, inspection type, and time of day
pass_rate_comparison = clean.groupby(['BOROUGH', 'INSPECTION_TYPE', 
                                         'TIME_OF_DAY'])['passing'].mean().reset_index()

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)

# Visualize the passing rates
plt.figure(figsize=(14, 7))
sns.barplot(data=pass_rate_comparison, x='BOROUGH', y='passing', hue='TIME_OF_DAY', ci=None)
plt.title('Passing Rates by Time of Day and Borough')
plt.ylabel('Passing Rate')
plt.xlabel('Borough')
plt.legend(title='Time of Day')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust the plot to ensure everything fits without overlapping
plt.show()
```

**Perform a formal hypothesis test to confirm the observations from your visualization.**
```{python}
# Load the dataset
clean = pd.read_csv('./data/cleaned_rodent_inspection.csv')

# Assuming the inspection date and result columns are named 'INSPECTION_DATE' and 'RESULT'
# Convert inspection date to datetime
clean['INSPECTION_DATE'] = pd.to_datetime(clean['INSPECTION_DATE'])

# Create 'TIME_OF_DAY' category based on the hour of the inspection
clean['TIME_OF_DAY'] = clean['INSPECTION_DATE'].dt.hour.apply(lambda x: 'Morning' 
                                                              if x < 12 
                                                              else 'Afternoon')

# Define criteria for 'passing'
clean['passing'] = clean['RESULT'].apply(lambda x: 1 if x == 'Passed' else 0)

# Perform hypothesis testing for each borough
results = []

for borough in clean['BOROUGH'].unique():
    # Subset data for the borough
    borough_data = clean[clean['BOROUGH'] == borough]
    
    # Create contingency table for passing rates by time of day
    contingency_table = pd.crosstab(borough_data['passing'], 
                                    borough_data['TIME_OF_DAY'])
    
    # Perform the Chi-Squared Test
    try:
        chi2, p, dof, expected = chi2_contingency(contingency_table)
        result = {
            'Borough': borough,
            'Chi-Squared': chi2,
            'P-Value': p,
            'Degrees of Freedom': dof
        }
    except ValueError as e:
        result = {
            'Borough': borough,
            'Error': str(e)
        }

    # Append the result
    results.append(result)

# Save the updated dataset
clean.to_csv('./data/cleaned_rodent_inspection.csv', index=False)

# Convert the results into a DataFrame for display
results_df = pd.DataFrame(results)
results_df
```

From the dataframe output, if we use $\alpha$ = 0.05, we can see that Brooklyn, Manhattan, Bronx, and Queens have a p-value less than $\alpha$. Therefore, we reject the null hypothesis and say that there is a statistically significant association between the time of day and inspection results within the boroughs. In the case of Staten Island, we fail to reject the null hypothesis and say that there is no association. This gives us insight on how inspection outcomes vary by time of day in different boroughs.

## Data analytics

**Aggregate the inspections by zip code to create a dataset with five columns. The first three columns are `zipcode`; `n_initial`, the count of the initial inspections in that zipcode; and  `n_initpass`, the number of initial inspections with a passing result in that zipcode. The other two variables are `n_compliance` and `n_comppass`, the counterpart for compliance inspections.**

```{python}
# Aggregate inspections by zip code

# Filtering data for 'Initial' and 'Compliance' inspections
initial_inspections = clean[clean['INSPECTION_TYPE'] == 'Initial']
compliance_inspections = clean[clean['INSPECTION_TYPE'] == 'Compliance']

# Aggregating data for initial inspections
n_initial = initial_inspections.groupby('ZIP_CODE')['JOB_TICKET_OR_WORK_ORDER_ID']\
                               .count().rename('n_initial')
n_initpass = initial_inspections[initial_inspections['passing'] == 1]\
                                 .groupby('ZIP_CODE')['JOB_TICKET_OR_WORK_ORDER_ID']\
                                 .count().rename('n_initpass')

# Aggregating data for compliance inspections
n_compliance = compliance_inspections.groupby('ZIP_CODE')['JOB_TICKET_OR_WORK_ORDER_ID']\
                                     .count().rename('n_compliance')
n_comppass = compliance_inspections[compliance_inspections['passing'] == 1]\
                                    .groupby('ZIP_CODE')['JOB_TICKET_OR_WORK_ORDER_ID']\
                                    .count().rename('n_comppass')

# Combining the results into one dataframe
aggregated_data = pd.concat([n_initial, n_initpass, n_compliance, n_comppass], \
                             axis=1).fillna(0).reset_index()

aggregated_data = aggregated_data.astype(int)
aggregated_data.head()
```

**Add a variable to your dataset, `n_sighting`, which represent the number of rodent sightings from the 311 service request data in the same 2022-2023 period.**
```{python}
sighting = pd.read_csv('./data/rodent_2022-2023.csv')

# Count the number of rodent sightings in each zip code
n_sighting = sighting.groupby('Incident Zip')['Unique Key'].count().rename('n_sighting')

# Merge this count with the aggregated dataset
aggregated_sightings = pd.merge(aggregated_data, n_sighting, how='left', 
                                left_on='ZIP_CODE', right_index=True)

# Fill NaN values with 0 for zip codes with no recorded sightings
aggregated_sightings['n_sighting'] = aggregated_sightings['n_sighting']\
                                     .fillna(0).astype(int)

aggregated_sightings.head()
```

**Merge your dataset with the simple zipcode table in package `uszipcode` by zipcode to obtain demographic and socioeconomic variables at the zipcode level.**
```{python}
search = SearchEngine()

# Function to fetch demographic and socioeconomic data for a given zip code
def fetch_zipcode_data(zip_code):
    zipcode_info = search.by_zipcode(str(zip_code))
    return zipcode_info.to_dict() if zipcode_info else None

# Fetching data for each unique zip code in the aggregated dataset
zipcode_data = []
for zip_code in aggregated_sightings['ZIP_CODE']:
    data = fetch_zipcode_data(zip_code)
    if data:
        zipcode_data.append(data)

# Remove None entries from the data
zipcode_data = [z for z in zipcode_data if z]

# Creating a DataFrame from the fetched data
zipcode_df = pd.DataFrame(zipcode_data)

def pretty_print(df, max_row=10, max_col=5):
    # Choose which columns to display
    df = df.iloc[:, :max_col]  # Limiting the number of columns to the first 5
    # Use a more compact floating point representation if needed
    with pd.option_context('display.float_format', '{:0.2f}'.format):
        print(df.head(max_row))

pretty_print(zipcode_df)

# Print the columns of the zipcode_df DataFrame vertically
for column in zipcode_df.columns:
    print(column)
```

**Build a binomial regression for the passing rate of initial inspections at the zipcode level. Assess the goodness-of-fit of your model. Summarize your results to a New Yorker who is not data science savvy.**

```{python}
# Creating the passing rate variable for initial inspections
# The passing rate is calculated as n_initpass / n_initial
aggregated_sightings['initial_pass_rate'] = (aggregated_sightings['n_initpass'] / 
                                             aggregated_sightings['n_initial'])

# Convert ZIP_CODE to strings in both DataFrames
aggregated_sightings['ZIP_CODE'] = aggregated_sightings['ZIP_CODE'].astype(str)
zipcode_df['zipcode'] = zipcode_df['zipcode'].astype(str)

# Merge the DataFrames on ZIP codes
merged_df = pd.merge(aggregated_sightings, zipcode_df, left_on='ZIP_CODE',
                     right_on='zipcode', how='left')

# Handle NaN or infinite values
merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
merged_df.dropna(subset=['n_sighting', 'population_density', 'median_household_income', 
                         'initial_pass_rate', 'median_home_value'], inplace=True)

# Predictor Variables
X = merged_df[['n_sighting', 'population_density', 
               'median_household_income', 'median_home_value']]

X = sm.add_constant(X)  # Adding a constant term for the regression

# Response Variable
y = merged_df['initial_pass_rate']

# Fit binomial regression model
model = sm.GLM(y, X, family=sm.families.Binomial())
result = model.fit()

# Display the results
print(result.summary())
```

**Analysis:**

- Pseudo R-Squ.: The value indicates the model's explanatory power. Our value of 0.002985 is very low and suggests that the model does not explain much of the variation in response variable.
- From the coefficients table, P>|z| will explain if the predictor is statistically significant. If we use $\alpha$ = 0.05, we can say that all of the predictors are not statistically significant. This is because their p-value is greater than our $\alpha$ level.
- In summary, our predictors do not have a significant impact on the passing rate in initial inspections. 

##  Research Question

   Now you know the data quite well. Come up with a research
   question of interest that can be answered by the data, which
   could be analytics or visualizations. Perform the needed
   analyses and answer your question.
 
**How do demographic factors like population density or socioeconomic factors like average income correlate with rodent sightings?**

```{python}
# Convert ZIP_CODE to string in both DataFrames
aggregated_sightings['ZIP_CODE'] = aggregated_sightings['ZIP_CODE'].astype(str)
zipcode_df['zipcode'] = zipcode_df['zipcode'].astype(str)

# Step 1: Merge the Datasets
merged_df = pd.merge(
    aggregated_sightings,
    zipcode_df,
    left_on='ZIP_CODE', 
    right_on='zipcode', 
    how='inner'
)

# Step 2: Correlation Analysis
correlation = merged_df[['n_sighting', 'population_density', \
                         'median_household_income']].corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Step 3: Visualization
# Scatter plot for population density and rodent sightings
sns.scatterplot(x='population_density', y='n_sighting', data=merged_df)
plt.title('Population Density vs. Rodent Sightings')
plt.show()

# Scatter plot for average income and rodent sightings
sns.scatterplot(x='median_household_income', y='n_sighting', data=merged_df)
plt.title('Median Household Income vs. Rodent Sightings')
plt.show()
```

From the correlation heatmap, we have three variables:

1. `n_sighting`: Number of rodent sightings.
2. `population_density`: Number of individuals living per unit of area.
3. `median_household_income`: Median income.

Between `n_sighting` and `population_density`, we see that there is a possible correlation of 0.4. It suggests that as the population density increases, the number of rodent sightings will tend to increase as well. In denser areas, they are subject to more waste production which creates more food availability for rodents. 

We also see a negative correlation of -0.37 between `median_household_income` and `n_sighting`. It suggests that areas with a higher income tend to have fewer rodent sightings and in turn, fewer inspections. It could be due to a variety of factors such as better waste management, more resources for pest control, or less overcrowding.

**Population Density vs. Sightings**:

- There are plenty of observations that come from an area with lower population densities.
- As mentioned before, we see the trend of sightings increase as the population density increases. 
- There is also a wide spread of rodent sightings across all levels of population density.

**Median Household Income vs. Sightings**:

- There are many values towards the lower end of income with a large cluster of rodent sightings. 
- There are outliers towards this lower end with an extreme number of rodent sightings. 
- Plot suggests a potential socioeconomic component to rodent infestations where wealthier areas have fewer sightings. 

## References 

* [GeoPy reference](https://stackoverflow.com/questions/60928516/get-address-from-given-coordinate-using-python)
* [PLUTO data](https://data.cityofnewyork.us/City-Government/Primary-Land-Use-Tax-Lot-Output-PLUTO-/64uk-42ks/data)
* [Rodent Inspection data](https://data.cityofnewyork.us/Health/Rodent-Inspection/p937-wjvj/about_data)